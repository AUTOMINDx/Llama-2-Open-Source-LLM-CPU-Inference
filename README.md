# Running Open-Source LLMs on CPU Inference for DocumentÂ Q&A

### Clearly explained step-by-step guide on using C Transformers, GGML, and LangChain for running LLM Python applications on CPU instances
___
## Context


___
## Tools


___
## Files

___

## References
- https://github.com/marella/ctransformers
- https://huggingface.co/TheBloke
- https://python.langchain.com/en/latest/integrations/ctransformers.html
- https://python.langchain.com/en/latest/modules/models/llms/integrations/ctransformers.html
- https://python.langchain.com/docs/ecosystem/integrations/ctransformers
- https://github.com/rustformers/llm/blob/main/crates/ggml/README.md
- https://huggingface.co/TheBloke/MPT-7B-Instruct-GGML
